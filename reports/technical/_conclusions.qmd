We developed a fully reproducible pipeline covering:

1. **Data Ingestion & Preparation**  
   - Loaded raw RDS rasters and planting/survey tables via `load_data.py`.  
   - Cleaned static features (`preprocess_features.py`) and extracted per-site time series (`pivot_data.py`).  
   - Created consistent train/test datasets with `data_split.py`.

2. **Model Development & Benchmarking**  
   - Phase 1: Trained and compared classical classifiers (Logistic Regression, Random Forest, XGBoost) using fixed-length features.  
   - Phase 2: Trained GRU and LSTM networks on temporal sequences, demonstrating the benefit of sequence data.

3. **Evaluation & Results Interpretation**  
   - Employed grouped 5-fold CV to avoid spatial leakage, optimized F1 for model selection, and reported Precision, Recall, F1, and AUC on held-out data.  
   - XGBoost emerged as the top static model; GRU showed promise for temporal modeling.

4. **Operational Recommendations**  
   - **Use XGBoost models as a reference** for identifying high-risk sites, rather than for direct operational deployment.  
   - **Explore GRU models** as proof-of-concept for temporal analysis, acknowledging their current performance limitations.  
   - **Treat all model outputs as advisory** insights; validate predictions with local surveys before taking field action.  
   - **Enhance data** through expanded Year-7 surveys and new covariates (soil, LiDAR).  
   - **Automate retraining** via Makefile and CI to adapt to new data.

By integrating data engineering, machine learning, and reproducible DevOps practices, this workflow provides a scalable tool for early afforestation monitoring and resource prioritization.