This phase of modeling began with three classical machine learning models: Logistic Regression (as a baseline), Random Forest, and Gradient Boosting Machines. To better capture the temporal structure of the remote sensing time series, two recurrent neural networks—GRU and LSTM—were subsequently developed. This section provides detailed descriptions of each model's architecture and the rationale for their selection. Methods for training, tuning, and evaluating model performance will also be thoroughly outlined.

### Classial Modelling

#### Logistic Regression

Logistic regression is a generalized linear model widely used for binary classification tasks, valued for its simplicity and interpretability. It models the **log-odds** of the probability, or the **logit** that a given record belongs to class 1 as a linear combination of the input features [@enwiki:logreg]:

$$
\log\left( \frac{p}{1 - p} \right) = \boldsymbol{\beta}_0 + \boldsymbol{\beta}^\top \mathbf{x}_i,
\quad i = 1, \dots, n
$$ {#eq-logit}

Here,

-   $n$ denotes the sample size,\
-   $\mathbf{x}_i = [x_{i1}, x_{i2}, \dots, x_{iD}]$ is the $D$-dimensional feature vector for the $i$th observation (e.g., site-level features and aggregated vegetation indices),\
-   $p$ is the probability that the target label $y_i$ corresponds to the high survival class: $p = P(y_i = 1 \mid \mathbf{x}_i)$

The coefficient vector $\boldsymbol{\beta} = [\beta_1, \beta_2, \dots, \beta_D]$ represents the influence of the features on each prediction. The $j$th entry of $\boldsymbol{\beta}$ corresponds to the change in the log-odds associated with a one-unit increase in the $j$th feature, holding all other features constant.

An optimal estimate of $\beta$ is determined by minimizing the **cross-entropy loss**:

$$
\mathcal{L} = -\frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log(\hat p_i) + (1 - y_i) \log(1 - \hat p_i) \right],
$$ {#eq-logloss}

Where $\hat p_i$ is the estimated class probability obtained from the inverse of @eq-logit, which can be shown to be the **sigmoid function**:

$$
\hat p_i = \sigma(\boldsymbol{\beta}_0 + \boldsymbol{\beta}^\top \mathbf{x}_i) = \frac{1}{1 + \exp\left( -(\boldsymbol{\beta}_0 + \boldsymbol{\beta}^\top \mathbf{x}_i) \right)}
$$ {#eq-sigmoid}

These probabilistic predictions can be converted to binary class labels by applying a specified decision threshold, typically 0.5. Model performance across different thresholds can be evaluated using ROC and precision–recall (PR) curves, which are discussed in @sec-metrics.

Overall, logistic regression provides an interpretable, statistically grounded baseline and serves as a proxy for the classical statistical modeling used prior to this analysis. To demonstrate the value of more sophisticated machine learning models in predicting survival rates, any subsequent models should achieve performance that exceeds that of logistic regression.

#### Tree-Based Modelling {#sec-tree_models}

Many high-performing machine learning models are composed of simple, rule-based structures known as decision trees. These models make predictions by recursively partitioning the input dataset into distinct regions based on selected features and threshold values. An example of a decision tree is shown in @fig-decision_tree.

![A simple example of a Decision Tree with a depth of 2. The predictions $Y$ are made by comparing selected features $X_A$ and $X_B$ via comparison with threshold values $T_1$ and $T_2$.](../../img/decision_tree.png){#fig-decision_tree width="70%"}

Each internal node in the tree represents a decision based on a specific feature and a corresponding threshold, and each leaf node corresponds to a unique subset of the data, defined by the path of decision rules leading to it. In binary classification, the majority label of samples in a leaf node is used as the prediction, but for regression, the mean of the target within the leaf node is given. Feature-threshold pairs are selected using a greedy algorithm: starting from the root node, the tree is grown iteratively by choosing the split that most effectively reduces the value of a given loss function. The cross-entropy loss defined in @eq-logloss is commonly used for binary classification tasks; however, Gini impurity is another frequently used criterion [@scikit-learn]. Alternatively, regression loss functions such as MSE can be used for Regression Tree tasks. Tree construction halts either when a leaf node contains only one class (resulting in zero loss for that subset) or when a predefined stopping criterion, such as the maximum depth, is met. See @sec-tree_hparams for guidance on selecting an appropriate maximum tree depth; choosing a higher max depth generally leads to a greater number of decisons and an overall more complex model.

#### Random Forest Classifier

The Random Forest model is an aggregate model composed of many decision trees, each trained on a bootstrapped subset of the training data and a randomly selected subset of the features. Typically, the maximum allowable depth for each tree in a Random Forest is quite high, resulting in individual trees that are often overfitted and exhibit high variance. However, this high variance is mitigated through aggregation: by combining the predictions of many diverse trees, the overall model can generalize effectively to unseen data. For binary classification tasks, the final prediction is determined by majority vote among the individual trees.

Although training Random Forests can be computationally intensive, each tree is trained independently, enabling efficient parallelization and scalability. Previous studies from @bergmuller2022predicting, have demonstrated that Random Forests perform well when using vegetation indices to predict canopy tree mortality. Becuase of this, this model was selected as a candidate for the present analysis.

#### Gradient Boosting Classifier

The Gradient Boosting model is a popular model that exists in a collection of 'boosting' models, which -unlike Random Forests- consists of a sequence of underfit and biased 'weak learner' models which converge to a high-performing 'strong learner' model when combined [@zhou2025ensemble]. This model was selected as a candidate model due to fast implementation and strong performance across a wide variety of machine learning tasks [@xgboost].

Convergence to a strong learner from a series of weak learners is performed by iteratively fitting a regression tree to the errors of the previous model estimate. To understand this, we firt define the per-sample the loss to be the negative of @eq-logloss evaluated for a particular class prediction $\hat p_i$:

$$
\ell_i(\hat p_i, y_i) = -\left[ y_i \log(\hat p_i) + (1 - y_i) \log(1 - \hat p_i) \right]
$$ {#eq-persample_loss}

The model outputs raw logit predictions $f_i(\mathbf{x}_i)$, which can be converted to probabilistic predictions via the sigmoid function shown in @eq-sigmoid:

$$
\hat p_i = \sigma(f_i(\mathbf{x}_i))
$$

The errors associated to each prediction are quantified by the **gradient** $g_i$ and **Hessian** $h_i$ of the loss with respect to the model estimate:

$$
g_i = \frac{\partial \ell_i}{\partial f(\mathbf{x}_i)} = \hat{p}_i - y_i\
$$ {#eq-gradient}

$$
h_i = \frac{\partial^2 \ell_i}{\partial f(\mathbf{x}_i)^2} = \hat{p}_i (1 - \hat{p}_i)
$$ {#eq-hessian}

##### Initialization

The model initializes with a constant prediction $f_0$ across all training sample, usually taken as the logit function (ie. the left-hand side of @eq-logit) evaluated over the proportion of samples with label 1:

$$
f_0 = \log\left( \frac{P(Y=1)}{1 - P(Y=1)} \right)
$$

##### Update step

To update the model prediction after initialization, a regression tree is fitted with the gradients given by @eq-gradient as the target predictor. Using Newton's method, the output for a particular leaf node $j$ is given by the sum of $g_i$ and $h_i$ for all samples that reach that leaf node.

$$
\omega_j^{(1)}  = \frac{\sum_{i \in j} g_i}{\sum_{i \in j} h_i}
$$ {#eq-gbm_weight}

The overall model prediction is then updated:

$$
f_1(\mathbf{x}_i) = f_0 + \eta \omega_{\mathbf{x}_i}^{(1)}
$$

Where $\omega_{\mathbf{x}_i}$ denotes the leaf node that sample $\mathbf{x}_i$ is assigned.

Where $\eta$ is a predefined **learning rate** which controls the degree to which each weak learner can make contributions to the overall model estimate. See @sec-tree_hparams for further details.

This update process is repeated iteratively, producing a final estimate of the log-odds which can be converted to a class probability and class labels through the same process as that of the logistic regression model:

$$
F(\mathbf{x}_i) = f_0 + \eta \sum_{k = 1}^{K} \omega_{\mathbf{x}_i}^{(k)}
$$

Where $K$ is the total number of iterations of the algorithm.

### Training and Tuning Classical Models

Most machine learning models involve a set of hyperparameters—values specified *a priori*—that govern model complexity and influence training behavior. Inappropriate hyperparameter choices can result in models that are either overly biased or unnecessarily complex, leading to poor generalization on unseen data. This section provides a detailed overview of the key hyperparameters for each candidate model in this analysis, along with the methodology used for their selection.

#### Regularization

In general, regularization involves a penalty to the loss function of that is proportional to the magnitude of the model parameters; stronger regularization leads to smaller parameters and more conservative predictions, which often aids in decreasing overfitting and variance. In Logistic Regression, this is implemented through an additional term in @eq-logloss:

$$
\mathcal{L} = -\frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log(\hat p_i) + (1 - y_i) \log(1 - \hat p_i) \right] + \lambda R(\boldsymbol{\beta})
$$ {#eq-logloss_reg}

Where $\lambda$ controls the strength of regularization (larger values lead to stronger regularization), and $R(\boldsymbol{\beta})$ is some function of the model parameter magnitude. In $L_1$ regularization, $R(\boldsymbol{\beta}) = \sum_j |\beta_j|$, and for $L_2$ regularization, $R(\boldsymbol{\beta}) = \sum_j (\beta_j)^2$. $L_1$ tends to decrease parameter values to 0 in a linear fashion, whereas $L_2$ causes parameters to asymptotically decrease towards, but never exactly to 0.

In the context of Gradient Boosting with XGBoost, regularization is applied to the loss function in the form:

$$
\mathcal{L} = -\frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log(\hat p_i) + (1 - y_i) \log(1 - \hat p_i) \right] + \lambda \left( T + R(\boldsymbol{\beta})\right)
$$ {#eq-logloss_reg_gbm}

Where $T$ is the number of leaves in the tree. Regularization is also applied to the weights directly, via modification of @eq-gbm_weight as implemented by @xgboost:

$$
\omega_j  = \frac{\sum_{i \in j} g_i}{\sum_{i \in j} h_i + \lambda}
$$ {#eq-gbm_weight_reg}

Generally, model performance varies logarithmically with $\lambda$, therefore it is advised that test values be sampled on a logarithmic scale when optimizing for performance.

#### Tree Hyperparameters {#sec-tree_hparams}

Nonparametric models such as Random Forest do not support regularization, therefore they must be optimized by controlling other hyperparameters. As stated in @sec-tree_models, the maximum depth directly controls model complexity by limiting the number of rules formulated by each tree, however other parameters such as minimum samples per leaf, the cost-complexity parameter $\alpha$, and the number of estimators may be adjusted as well [@scikit-learn]. For simplicity and to reduce the necessary training time, only maximum depth and number of estimators were controlled for in this analysis.

#### Random Search Cross-Validation

Given a candidate model and a set of variable hyperparameters, an optimization problem arises: what configuration of hyperparameters best optimizes model performance? Various methods exist, however this analysis used random search cross validation to tune the model hyperparameters. @fig-cross_validation illustrates the process of cross-validation.

![An example of four-fold cross-validation. A given model with a configuration of hyperparameters is trained four times, with one subset of the data left as a 'hold-out' validation subset to be used to test model performance. After this, the test scores are averaged. This is repeated for multiple configurations of hyperparameters, and the configuration that achieves the highest score may be selected for further evaluation. Many scoring metrics exist, depending on the use case and nature of the data. See @sec-error_metrics for further details regarding the metrics used in this analysis.](../../img/cross_validation.png)

By averaging over a hold-out validation subset, this method gives the advantage of simulating model performance in deployment; simply optimizing over the entire training set would likely lead to an overestimation of performance and failure to evaluate model overfitting. In **random search**, hyperparameters are randomly sampled according to a predefined distribution after each iteration of the cross-validaiton algorithm. **Grid search** is another popular method, in which all combinations of specified hyperparameter values are tested. However, this method becomes computationally exhaustive as the number of hyperparameters and test values increases. Therefore, random search was opted for in this analysis.

### Sequential Deep Learning Models

-   Classical models do not naturally handle sequential data
-   Literature indicates changes in VIs across time are vital to predicting survival rates
-   Problem: require large and diverse datasets.

#### Recursive Neural Network (RNN)

-   Inputs passed sequentially to model
-   Each input updates a hidden state via a linear combination: h_t = (W\_{xh} \* X_t + W\_{hh} \* h\_{t-1})
-   Hidden state represents memory data across time
-   Vanilla RNNs suffer from vanishing/exploding gradients (see section @sec-backpropagation)

#### Long-term Short Term Memory (LSTM)

-   Uses keep and forget gates to capture long term memory more effectively
-   Show equation for updating hidden state with image of cell

#### Gated Recurrent Unit (GRU)

-   Simplified form of LSTM, one combined gate for forget and keep
-   Show equation for updating hidden state with image of cell

#### Bidirectional RNNs

-   Add concatenated backward layer for encoding time in both directions

#### Fully Connected Neural Network (FCNN)

-   Linear transformation of input
-   Activation functions: control output scales and add nonlinearity (show ReLU function and justify use)

#### Final Model architecture

-   time series into GRU or LSTM
-   concatenate with site features and pass to FCNN
-   FCNN output goes to sigmoid function \* 100 to predict survival rate
-   Dropout Rate before prediction, show paper for evidence of improvement
-   hidden layers size, size of hidden state, all hyperparameters

### Training Deep Learning Models

-   Loss Function: MSE
-   Compute gradient of loss WRT to each parameter in model
-   Show equation for FCCN params
-   show time dependence for sequential parts
-   Show gradient descent equation for updating parameters
-   Mini-batch: Compute gradients on subsets of data per update step to reduce computational load
-   Repeat over succcessive epochs (epoch = 1 run through of data)

### Error Metrics {#sec-error_metrics}

#### Precision, Recall, $F_{\beta}$ Score

-   definition of each
-   justification of F1 over other Fbeta and direct accuracy

#### ROC and PR Curves

-   define TPR, FPR, and curves
-   AUROC, AUC to measure model performance
-   reference curves in results