We evaluate three model families of increasing complexity: a transparent baseline (**logistic regression**), non-linear ensembles (**random forest (RF)** and **gradient boosting machines (GBM)**), and sequence-aware deep networks (**long short-term memory (LSTM)** and **gated recurrent unit (GRU)** recurrent neural networks (RNNs)).

### Logistic Regression  
A fast, interpretable baseline that estimates survival odds and provides coefficient-based feature importance.

### Ensemble Trees: RF & GBM  
RF averages decorrelated trees to reduce variance; GBM builds trees sequentially to correct predecessor errors, jointly capturing non-linear interactions in vegetation indices [@bergmuller2022predicting].

### Sequential Deep Learning: LSTM & GRU RNNs  
If time permits, LSTM/GRU RNNs will model temporal patterns in multi-year vegetation-index series to detect survival trends [@paszke2019pytorch].
