{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "866b4937",
   "metadata": {},
   "source": [
    "# Environment Setup\n",
    "\n",
    "Follow these steps to set up and activate the development environment for this project:\n",
    "\n",
    "1. **Create the Environment**\n",
    "\n",
    "   Open a terminal in the project root directory and run one of the following commands:\n",
    "\n",
    "   - For the standard environment:\n",
    "     ```bash\n",
    "     conda env create -f environment-dev.yml\n",
    "     ```\n",
    "   - For GPU support (e.g., with Sockeye):\n",
    "     ```bash\n",
    "     conda env create -f environment-dev-gpu.yml\n",
    "     ```\n",
    "\n",
    "2. **Activate the Environment**\n",
    "\n",
    "   - For the standard environment:\n",
    "     ```bash\n",
    "     conda activate mds-afforest-dev\n",
    "     ```\n",
    "   - For GPU support:\n",
    "     ```bash\n",
    "     conda activate mds-afforest-dev-gpu\n",
    "     ```\n",
    "\n",
    "These commands will install all required dependencies for development.  \n",
    "**Note:** Ensure you have [conda](https://docs.conda.io/en/latest/miniconda.html) installed before proceeding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c3fe08",
   "metadata": {},
   "source": [
    "# Running the Scripts\n",
    "\n",
    "To run the scripts, ensure you are in the project directory and the environment is activated. You can then execute the scripts using make. You can find the available scripts in the `Makefile` located in the root directory of the project. The scripts are organized into different sections, such as data processing, model training, and evaluation.\n",
    "\n",
    "## Pre-requisites\n",
    "Before running the scripts, ensure you have:\n",
    "- Installed the required environment as described above.\n",
    "- All necessary data files are available in the expected directories. By default the raw data is not available in the repository, but you can download it from: [Google Drive Link](https://drive.google.com/file/d/1GengsSVG29m0wH9EET1oaVhadv48dgGj/view?usp=drive_link)\n",
    "- Place the data files in the `data/raw` directory of the project.\n",
    "\n",
    "## Load the Data\n",
    "To load the data into parquet format, you can use the provided script. Run the following command in the terminal:\n",
    "```bash\n",
    "make data/raw/raw_data.parquet: RAW_DATA_PATH=data/raw/AfforestationAssessmentDataUBCCapstone.rds\n",
    "```\n",
    "\n",
    "## Preprocess the Data\n",
    "To preprocess the data, you can use the following command:\n",
    "```bash\n",
    "make preprocess_features\n",
    "```\n",
    "\n",
    "## Pivot the data\n",
    "To pivot the data, you can use the following command:\n",
    "```bash\n",
    "make pivot_data\n",
    "```\n",
    "\n",
    "## Split the Data  \n",
    "To split the processed data into training and test sets:  \n",
    "```bash\n",
    "make data_split\n",
    "```  \n",
    "This will execute the `data_split.py` script to generate the train and test datasets in the specified directory.\n",
    "\n",
    "## Train the Models\n",
    "\n",
    "### Classical Machine Learning Models\n",
    "To train the models using the provided pipelines, run the following commands:\n",
    "\n",
    "- **Logistic Regression:**\n",
    "    ```bash\n",
    "    make logistic_regression_pipeline\n",
    "    ```\n",
    "    This will train a logistic regression model and save it to `models/logistic_regression.joblib`.\n",
    "\n",
    "- **Random Forest:**\n",
    "    ```bash\n",
    "    make random_forest_pipeline\n",
    "    ```\n",
    "    This will train a random forest model and save it to `models/` directory.\n",
    "\n",
    "- **Gradient Boosting:**\n",
    "    ```bash\n",
    "    make gradient_boosting_pipeline\n",
    "    ```\n",
    "    This will train a gradient boosting model and save it to `models/` directory.\n",
    "- **All models**:\n",
    "    ```bash\n",
    "    make all_classical_models\n",
    "    ```\n",
    "    This will train all the models defined in the `Makefile` and save them to the `models/` directory.\n",
    "- **Fine-tune the models**:\n",
    "    ```bash\n",
    "    make tune_classical_models\n",
    "    ```\n",
    "    This will fine-tune the models using the `fine_tune.py` script and save the best models to the `models/` directory. This might take some time depending on the dataset size and the number of hyperparameter combinations.\n",
    "\n",
    "### Deep Learning Models (RNNs)\n",
    "\n",
    "\n",
    "- **Prepare data for RNN models:**  \n",
    "    To generate the time series train and test datasets required for RNN models, run:  \n",
    "    ```bash\n",
    "    make data_for_RNN_models\n",
    "    ```\n",
    "    This will create `data/processed/train_lookup.parquet` and `data/processed/test_lookup.parquet` for use in RNN training.\n",
    "\n",
    "    To split the processed data specifically for RNN models, run:  \n",
    "    ```bash\n",
    "    make data_split_RNN\n",
    "    ```\n",
    "    This will generate the appropriate train and test splits for RNN-based workflows.\n",
    "\n",
    "- **Train RNN models:**\n",
    "    To train the RNN models, you can use the following command:\n",
    "    ```bash\n",
    "    make rnn_pipeline RNN_TYPE=GRU RNN_PIPELINE_PATH=models/gru_no_sites.pth    \n",
    "    ```\n",
    "    This will train the RNN model and save it to `models/` directory.\n",
    "\n",
    "## Running tests\n",
    "To run the tests for the project, you can use the following command:\n",
    "\n",
    "```bash\n",
    "make test\n",
    "```\n",
    "\n",
    "## Clean Up\n",
    "\n",
    "To clean up generated data and models, you can use the following commands:\n",
    "\n",
    "- **Clean data files:**\n",
    "    ```bash\n",
    "    make clean_data\n",
    "    ```\n",
    "    This will remove the raw, interim, and processed data files and recreate the necessary directories with `.gitkeep` files.\n",
    "\n",
    "- **Clean model files:**\n",
    "    ```bash\n",
    "    make clean_models\n",
    "    ```\n",
    "    This will remove all files in the `models` directory.\n",
    "- **Clean all generated files:**\n",
    "    ```bash\n",
    "    make clean_all\n",
    "    ```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
