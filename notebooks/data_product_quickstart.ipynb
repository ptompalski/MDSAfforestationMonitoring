{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "866b4937",
   "metadata": {},
   "source": [
    "# Environment Setup\n",
    "\n",
    "Follow these steps to set up and activate the development environment for this project:\n",
    "\n",
    "1. **Create the Environment**\n",
    "\n",
    "   Open a terminal in the project root directory and run one of the following commands:\n",
    "\n",
    "   - For the standard environment:\n",
    "     ```bash\n",
    "     conda env create -f environment-dev.yml\n",
    "     ```\n",
    "   - For GPU support (e.g., with Sockeye):\n",
    "     ```bash\n",
    "     conda env create -f environment-dev-gpu.yml\n",
    "     ```\n",
    "\n",
    "2. **Activate the Environment**\n",
    "\n",
    "   - For the standard environment:\n",
    "     ```bash\n",
    "     conda activate mds-afforest-dev\n",
    "     ```\n",
    "   - For GPU support:\n",
    "     ```bash\n",
    "     conda activate mds-afforest-dev-gpu\n",
    "     ```\n",
    "\n",
    "These commands will install all required dependencies for development.  \n",
    "**Note:** Ensure you have [conda](https://docs.conda.io/en/latest/miniconda.html) installed before proceeding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c3fe08",
   "metadata": {},
   "source": [
    "# Running the Scripts\n",
    "\n",
    "To run the scripts, ensure you are in the project directory and the environment is activated. You can then execute the scripts using make. You can find the available scripts in the `Makefile` located in the root directory of the project. The scripts are organized into different sections, such as data processing, model training, and evaluation.\n",
    "\n",
    "## Pre-requisites\n",
    "Before running the scripts, ensure you have:\n",
    "- Installed the required environment as described above.\n",
    "- All necessary data files are available in the expected directories. By default the raw data is not available in the repository, but you can download it from: [Google Drive Link](https://drive.google.com/file/d/1GengsSVG29m0wH9EET1oaVhadv48dgGj/view?usp=drive_link)\n",
    "- Place the data files in the `data/raw` directory of the project.\n",
    "\n",
    "## Load the Data\n",
    "To load the data into parquet format, you can use the provided script. Run the following command in the terminal:\n",
    "```bash\n",
    "make data/raw/raw_data.parquet RAW_DATA_PATH=data/raw/AfforestationAssessmentDataUBCCapstone.rds\n",
    "```\n",
    "\n",
    "## Preprocess the Data\n",
    "To preprocess the data, you can use the following command:\n",
    "```bash\n",
    "make preprocess_features\n",
    "```\n",
    "\n",
    "# Data Processing for Classical Models\n",
    "\n",
    "## Set the Threshold\n",
    "You can set the threshold for the pivoting operation by defining the `THRESHOLD` variable. This variable determines the high and low survival rates. You can set it in the command line when running the pivoting script, as shown below.\n",
    "```bash\n",
    "THRESHOLD=0.7\n",
    "```\n",
    "\n",
    "## Pivot the data\n",
    "To pivot the data, you can use the following command:\n",
    "```bash\n",
    "make pivot_data THRESHOLD=${THRESHOLD}\n",
    "```\n",
    "\n",
    "## Split the Data  \n",
    "To split the processed data into training and test sets:  \n",
    "```bash\n",
    "make data_split THRESHOLD=${THRESHOLD}\n",
    "```  \n",
    "This will execute the `data_split.py` script to generate the train and test datasets in the specified directory.\n",
    "\n",
    "\n",
    "## Execute all Preprocessing for Classical Models\n",
    "\n",
    "**To execute all of these commands at once:**\n",
    "\n",
    "```bash\n",
    "make data_for_classical_models THRESHOLD=${THRESHOLD}\n",
    "```\n",
    "\n",
    "\n",
    "# Data Processing for RNN Models\n",
    "\n",
    "## Split cleaned data\n",
    "The RNN model does not use a threshold, so it must be split at the interim processing stage:\n",
    "```bash\n",
    "make data_split_RNN\n",
    "```\n",
    "This splits the partially cleaned dataset into training and testing subsets.\n",
    "\n",
    "## Generate Training Sequence Data\n",
    "To generate the training time series data for the RNN modelling:\n",
    "```bash\n",
    "make time_series_train_data\n",
    "```\n",
    "This will execute the `get_time_series.py` script to generate the training lookup table, the sequences and the `norm_stats.json` file used for standard scaling.\n",
    "\n",
    "## Generate Testing and Validation Sequence Data\n",
    "\n",
    "To generate the test and validation time series data for the RNN modelling:\n",
    "```bash\n",
    "make time_series_test_data\n",
    "```\n",
    "This will execute the `get_time_series.py` to generate the training and validation lookup tables and sequences, and use `norm_stats.json` to standardize features.\n",
    "\n",
    "\n",
    "## Execute all Preprocessing for RNN Models\n",
    "\n",
    "**To execute all of these commands at once:**\n",
    "\n",
    "```bash\n",
    "make data_for_RNN_models\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd32e94",
   "metadata": {},
   "source": [
    "\n",
    "## Train the Models\n",
    "\n",
    "### Classical Machine Learning Models\n",
    "To train the models using the provided pipelines, run the following commands:\n",
    "\n",
    "- **Logistic Regression:**\n",
    "    ```bash\n",
    "    make logistic_regression_pipeline\n",
    "    ```\n",
    "    This will train a logistic regression model and save it to `models/logistic_regression.joblib`.\n",
    "\n",
    "- **Random Forest:**\n",
    "    ```bash\n",
    "    make random_forest_pipeline\n",
    "    ```\n",
    "    This will train a random forest model and save it to `models/` directory.\n",
    "\n",
    "- **Gradient Boosting:**\n",
    "    ```bash\n",
    "    make gradient_boosting_pipeline\n",
    "    ```\n",
    "    This will train a gradient boosting model and save it to `models/` directory.\n",
    "- **All models**:\n",
    "    ```bash\n",
    "    make all_classical_models\n",
    "    ```\n",
    "    This will train all the models defined in the `Makefile` and save them to the `models/` directory.\n",
    "\n",
    "## Fine-tune the classical models\n",
    "You can customize hyperparameters for tuning by setting the following variables in your command:\n",
    "\n",
    "- `TUNING_METHOD`: Specify the tuning method (e.g., `grid` or `random`).\n",
    "- `PARAM_GRID`: Define the parameter grid as a string (e.g., `\"{'C':[0.1,1,10]}\"`).\n",
    "- `NUM_ITER`: Number of iterations for randomized search.\n",
    "- `NUM_FOLDS`: Number of cross-validation folds.\n",
    "- `SCORING`: Scoring metric (e.g., `accuracy`, `f1`).\n",
    "- `RANDOM_STATE`: Random seed for reproducibility.\n",
    "- `RETURN_RESULTS`: Set to `True` to return full results.\n",
    "\n",
    "\n",
    "### Set up arguments for tuning\n",
    "```bash\n",
    "TUNING_METHOD=random\n",
    "PARAM_GRID={}\n",
    "NUM_ITER=20\n",
    "NUM_FOLDS=5\n",
    "SCORING=f1\n",
    "RANDOM_STATE=42\n",
    "RETURN_RESULTS=True\n",
    "```\n",
    "\n",
    "You can tune each classical model separately using the following commands:\n",
    "\n",
    "- **Tune Gradient Boosting:**\n",
    "    ```bash\n",
    "    make tune_gbm \\\n",
    "        TUNING_METHOD=${TUNING_METHOD} \\\n",
    "        PARAM_GRID=${PARAM_GRID} \\\n",
    "        NUM_ITER=${NUM_ITER} \\\n",
    "        NUM_FOLDS=${NUM_FOLDS} \\\n",
    "        SCORING=${SCORING} \\\n",
    "        RANDOM_STATE=${RANDOM_STATE} \\\n",
    "        RETURN_RESULTS=${RETURN_RESULTS} \\\n",
    "        THRESHOLD_PCT=${THRESHOLD_PCT}\n",
    "    ```\n",
    "\n",
    "- **Tune Random Forest:**\n",
    "    ```bash\n",
    "    make tune_rf \\\n",
    "        TUNING_METHOD=${TUNING_METHOD} \\\n",
    "        PARAM_GRID=${PARAM_GRID} \\\n",
    "        NUM_ITER=${NUM_ITER} \\\n",
    "        NUM_FOLDS=${NUM_FOLDS} \\\n",
    "        SCORING=${SCORING} \\\n",
    "        RANDOM_STATE=${RANDOM_STATE} \\\n",
    "        RETURN_RESULTS=${RETURN_RESULTS} \\\n",
    "        THRESHOLD_PCT=${THRESHOLD_PCT}\n",
    "    ```\n",
    "\n",
    "- **Tune Logistic Regression:**\n",
    "    ```bash\n",
    "    make tune_lr \\\n",
    "        TUNING_METHOD=${TUNING_METHOD} \\\n",
    "        PARAM_GRID=${PARAM_GRID} \\\n",
    "        NUM_ITER=${NUM_ITER} \\\n",
    "        NUM_FOLDS=${NUM_FOLDS} \\\n",
    "        SCORING=${SCORING} \\\n",
    "        RANDOM_STATE=${RANDOM_STATE} \\\n",
    "        RETURN_RESULTS=${RETURN_RESULTS} \\\n",
    "        THRESHOLD_PCT=${THRESHOLD_PCT}\n",
    "    ```\n",
    "\n",
    "- **Tune all models:**\n",
    "    ```bash\n",
    "    make tune_classical_models \\\n",
    "        TUNING_METHOD=${TUNING_METHOD} \\\n",
    "        PARAM_GRID=${PARAM_GRID} \\\n",
    "        NUM_ITER=${NUM_ITER} \\\n",
    "        NUM_FOLDS=${NUM_FOLDS} \\\n",
    "        SCORING=${SCORING} \\\n",
    "        RANDOM_STATE=${RANDOM_STATE} \\\n",
    "        RETURN_RESULTS=${RETURN_RESULTS} \\\n",
    "        THRESHOLD_PCT=${THRESHOLD_PCT}\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc5e4d3",
   "metadata": {},
   "source": [
    "\n",
    "## Deep Learning Models (RNNs)\n",
    "\n",
    "### Set up arguments for RNN model\n",
    "\n",
    "Set the following environment variables to configure your RNN model architecture and training:\n",
    "\n",
    "- `INPUT_SIZE`: Number of input features per time step (default: 12)\n",
    "- `HIDDEN_SIZE`: Number of hidden units in the RNN (default: 16)\n",
    "- `SITE_FEATURES_SIZE`: Number of site-level features to concatenate (default: 4)\n",
    "- `RNN_TYPE`: Type of RNN cell to use (`LSTM`, `GRU`)\n",
    "- `NUM_LAYERS`: Number of stacked RNN layers (default: 1)\n",
    "- `DROPOUT_RATE`: Dropout rate between RNN layers (default: 0.2)\n",
    "- `CONCAT_FEATURES`: Whether to concatenate site features (`True` or `False`)\n",
    "- `RNN_PIPELINE_PATH`: Path to save the trained RNN pipeline\n",
    "\n",
    "```bash\n",
    "    INPUT_SIZE=12 \n",
    "    HIDDEN_SIZE=16 \n",
    "    SITE_FEATURES_SIZE=4\n",
    "    RNN_TYPE=LSTM\n",
    "    NUM_LAYERS=2\n",
    "    DROPOUT_RATE=0.3\n",
    "    CONCAT_FEATURES=True\n",
    "    RNN_PIPELINE_PATH=models/rnn_model.pth\n",
    "```\n",
    "\n",
    "### Set up arguments for RNN training\n",
    "Set the following environment variables to configure your RNN training:\n",
    "\n",
    "- `LR`: Learning rate for the optimizer (default: 0.01)\n",
    "- `BATCH_SIZE`: Batch size for training (default: 64)\n",
    "- `EPOCHS`: Number of training epochs (default: 10)\n",
    "- `PATIENCE`: Early stopping patience (default: 5)\n",
    "- `NUM_WORKERS`: Number of workers for data loading (default: 0)\n",
    "- `SITE_COLS`: Comma-separated list of site-level feature columns (default: Density,Type_Conifer,Type_Decidous,Age)\n",
    "- `SEQ_COLS`: Comma-separated list of sequential feature columns (default: NDVI,SAVI,MSAVI,EVI,EVI2,NDWI,NBR,TCB,TCG,TCW,log_dt,neg_cos_DOY)\n",
    "- `TRAINED_RNN_OUTPUT_PATH`: Path to save the trained RNN model outputs\n",
    "\n",
    "```bash\n",
    "    LR=0.01\n",
    "    BATCH_SIZE=64\n",
    "    EPOCHS=10\n",
    "    PATIENCE=5\n",
    "    NUM_WORKERS=0\n",
    "    SITE_COLS=Density,Type_Conifer,Type_Decidous,Age\n",
    "    SEQ_COLS=NDVI,SAVI,MSAVI,EVI,EVI2,NDWI,NBR,TCB,TCG,TCW,log_dt,neg_cos_DOY\n",
    "    TRAINED_RNN_OUTPUT_PATH=models/trained_rnn_model.pth\n",
    "```\n",
    "\n",
    "- **Create RNN model:**  \n",
    "    To train the RNN model, run the following command:\n",
    "    ```bash\n",
    "    make rnn_model \\\n",
    "        INPUT_SIZE=${INPUT_SIZE} \\\n",
    "        HIDDEN_SIZE=${HIDDEN_SIZE} \\\n",
    "        SITE_FEATURES_SIZE=${SITE_FEATURES_SIZE} \\\n",
    "        RNN_TYPE=${RNN_TYPE} \\\n",
    "        NUM_LAYERS=${NUM_LAYERS} \\\n",
    "        DROPOUT_RATE=${DROPOUT_RATE} \\\n",
    "        CONCAT_FEATURES=${CONCAT_FEATURES} \\\n",
    "        RNN_PIPELINE_PATH=${RNN_PIPELINE_PATH}\n",
    "    ```\n",
    "\n",
    "- **Train RNN model:**\n",
    "    To train the RNN model with the specified parameters, run:\n",
    "    ```bash\n",
    "    make rnn_training \\\n",
    "        LR=${LR} \\\n",
    "            BATCH_SIZE=${BATCH_SIZE} \\\n",
    "            EPOCHS=${EPOCHS} \\\n",
    "            PATIENCE=${PATIENCE} \\\n",
    "            NUM_WORKERS=${NUM_WORKERS} \\\n",
    "            SITE_COLS=${SITE_COLS} \\\n",
    "            SEQ_COLS=${SEQ_COLS} \\\n",
    "            RNN_PIPELINE_PATH=${RNN_PIPELINE_PATH} \\\n",
    "            TRAINED_RNN_OUTPUT_PATH=${TRAINED_RNN_OUTPUT_PATH}\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e98d2b",
   "metadata": {},
   "source": [
    "## Test\n",
    "To run the tests, you can use the following command:\n",
    "```bash\n",
    "make test\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb52d14",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Clean Up\n",
    "\n",
    "To clean up generated data and models, you can use the following commands:\n",
    "\n",
    "- **Clean data files:**\n",
    "    ```bash\n",
    "    make clean_data\n",
    "    ```\n",
    "    This will remove the raw, interim, and processed data files and recreate the necessary directories with `.gitkeep` files.\n",
    "\n",
    "- **Clean model files:**\n",
    "    ```bash\n",
    "    make clean_models\n",
    "    ```\n",
    "    This will remove all files in the `models` directory.\n",
    "- **Clean all generated files:**\n",
    "    ```bash\n",
    "    make clean_all\n",
    "    ```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
